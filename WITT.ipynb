{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This code is the implementation of WITT:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from glob import glob\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.utils.data as data\n",
    "from datetime import datetime\n",
    "from random import choice\n",
    "import random\n",
    "import logging\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# channel module\n",
    "class Channel(nn.Module):\n",
    "    \"\"\"\n",
    "    Currently the channel model is either error free, erasure channel,\n",
    "    rayleigh channel or the AWGN channel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, config):\n",
    "        super(Channel, self).__init__()\n",
    "        self.config = config\n",
    "        self.chan_type = args.channel_type\n",
    "        self.device = config.device\n",
    "        self.h = torch.sqrt(torch.randn(1) ** 2\n",
    "                            + torch.randn(1) ** 2) / 1.414\n",
    "        if config.logger:\n",
    "            config.logger.info('【Channel】: Built {} channel, SNR {} dB.'.format(\n",
    "                args.channel_type, args.multiple_snr))\n",
    "\n",
    "    def gaussian_noise_layer(self, input_layer, std, name=None):\n",
    "        device = input_layer.get_device()\n",
    "        noise_real = torch.normal(mean=0.0, std=std, size=np.shape(input_layer), device=device)\n",
    "        noise_imag = torch.normal(mean=0.0, std=std, size=np.shape(input_layer), device=device)\n",
    "        noise = noise_real + 1j * noise_imag\n",
    "        return input_layer + noise\n",
    "\n",
    "    def rayleigh_noise_layer(self, input_layer, std, name=None):\n",
    "        noise_real = torch.normal(mean=0.0, std=std, size=np.shape(input_layer))\n",
    "        noise_imag = torch.normal(mean=0.0, std=std, size=np.shape(input_layer))\n",
    "        noise = noise_real + 1j * noise_imag\n",
    "        h = torch.sqrt(torch.normal(mean=0.0, std=1, size=np.shape(input_layer)) ** 2\n",
    "                       + torch.normal(mean=0.0, std=1, size=np.shape(input_layer)) ** 2) / np.sqrt(2)\n",
    "        if self.config.CUDA:\n",
    "            noise = noise.to(input_layer.get_device())\n",
    "            h = h.to(input_layer.get_device())\n",
    "        return input_layer * h + noise\n",
    "\n",
    "\n",
    "    def complex_normalize(self, x, power):\n",
    "        pwr = torch.mean(x ** 2) * 2\n",
    "        out = np.sqrt(power) * x / torch.sqrt(pwr)\n",
    "        return out, pwr\n",
    "\n",
    "\n",
    "    def forward(self, input, chan_param, avg_pwr=False):\n",
    "        if avg_pwr:\n",
    "            power = 1\n",
    "            channel_tx = np.sqrt(power) * input / torch.sqrt(avg_pwr * 2)\n",
    "        else:\n",
    "            channel_tx, pwr = self.complex_normalize(input, power=1)\n",
    "        input_shape = channel_tx.shape\n",
    "        channel_in = channel_tx.reshape(-1)\n",
    "        L = channel_in.shape[0]\n",
    "        channel_in = channel_in[:L // 2] + channel_in[L // 2:] * 1j\n",
    "        channel_output = self.complex_forward(channel_in, chan_param)\n",
    "        channel_output = torch.cat([torch.real(channel_output), torch.imag(channel_output)])\n",
    "        channel_output = channel_output.reshape(input_shape)\n",
    "        if self.chan_type == 1 or self.chan_type == 'awgn':\n",
    "            noise = (channel_output - channel_tx).detach()\n",
    "            noise.requires_grad = False\n",
    "            channel_tx = channel_tx + noise\n",
    "            if avg_pwr:\n",
    "                return channel_tx * torch.sqrt(avg_pwr * 2)\n",
    "            else:\n",
    "                return channel_tx * torch.sqrt(pwr)\n",
    "        elif self.chan_type == 2 or self.chan_type == 'rayleigh':\n",
    "            if avg_pwr:\n",
    "                return channel_output * torch.sqrt(avg_pwr * 2)\n",
    "            else:\n",
    "                return channel_output * torch.sqrt(pwr)\n",
    "\n",
    "    def complex_forward(self, channel_in, chan_param):\n",
    "        if self.chan_type == 0 or self.chan_type == 'none':\n",
    "            return channel_in\n",
    "\n",
    "        elif self.chan_type == 1 or self.chan_type == 'awgn':\n",
    "            channel_tx = channel_in\n",
    "            sigma = np.sqrt(1.0 / (2 * 10 ** (chan_param / 10)))\n",
    "            chan_output = self.gaussian_noise_layer(channel_tx,\n",
    "                                                    std=sigma,\n",
    "                                                    name=\"awgn_chan_noise\")\n",
    "            return chan_output\n",
    "\n",
    "        elif self.chan_type == 2 or self.chan_type == 'rayleigh':\n",
    "            channel_tx = channel_in\n",
    "            sigma = np.sqrt(1.0 / (2 * 10 ** (chan_param / 10)))\n",
    "            chan_output = self.rayleigh_noise_layer(channel_tx,\n",
    "                                                    std=sigma,\n",
    "                                                    name=\"rayleigh_chan_noise\")\n",
    "            return chan_output\n",
    "\n",
    "\n",
    "    def noiseless_forward(self, channel_in):\n",
    "        channel_tx = self.normalize(channel_in, power=1)\n",
    "        return channel_tx\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loss function\n",
    "@torch.jit.script\n",
    "def create_window(window_size: int, sigma: float, channel: int):\n",
    "    '''\n",
    "    Create 1-D gauss kernel\n",
    "    :param window_size: the size of gauss kernel\n",
    "    :param sigma: sigma of normal distribution\n",
    "    :param channel: input channel\n",
    "    :return: 1D kernel\n",
    "    '''\n",
    "    coords = torch.arange(window_size, dtype=torch.float)\n",
    "    coords -= window_size // 2\n",
    "\n",
    "    g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n",
    "    g /= g.sum()\n",
    "\n",
    "    g = g.reshape(1, 1, 1, -1).repeat(channel, 1, 1, 1)\n",
    "    return g\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def _gaussian_filter(x, window_1d, use_padding: bool):\n",
    "    '''\n",
    "    Blur input with 1-D kernel\n",
    "    :param x: batch of tensors to be blured\n",
    "    :param window_1d: 1-D gauss kernel\n",
    "    :param use_padding: padding image before conv\n",
    "    :return: blured tensors\n",
    "    '''\n",
    "    C = x.shape[1]\n",
    "    padding = 0\n",
    "    if use_padding:\n",
    "        window_size = window_1d.shape[3]\n",
    "        padding = window_size // 2\n",
    "    out = F.conv2d(x, window_1d, stride=1, padding=(0, padding), groups=C)\n",
    "    out = F.conv2d(out, window_1d.transpose(2, 3), stride=1, padding=(padding, 0), groups=C)\n",
    "    return out\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def ssim(X, Y, window, data_range: float, use_padding: bool = False):\n",
    "    '''\n",
    "    Calculate ssim index for X and Y\n",
    "    :param X: images\n",
    "    :param Y: images\n",
    "    :param window: 1-D gauss kernel\n",
    "    :param data_range: value range of input images. (usually 1.0 or 255)\n",
    "    :param use_padding: padding image before conv\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    K1 = 0.01\n",
    "    K2 = 0.03\n",
    "    compensation = 1.0\n",
    "\n",
    "    C1 = (K1 * data_range) ** 2\n",
    "    C2 = (K2 * data_range) ** 2\n",
    "\n",
    "    mu1 = _gaussian_filter(X, window, use_padding)\n",
    "    mu2 = _gaussian_filter(Y, window, use_padding)\n",
    "    sigma1_sq = _gaussian_filter(X * X, window, use_padding)\n",
    "    sigma2_sq = _gaussian_filter(Y * Y, window, use_padding)\n",
    "    sigma12 = _gaussian_filter(X * Y, window, use_padding)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = compensation * (sigma1_sq - mu1_sq)\n",
    "    sigma2_sq = compensation * (sigma2_sq - mu2_sq)\n",
    "    sigma12 = compensation * (sigma12 - mu1_mu2)\n",
    "\n",
    "    cs_map = (2 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)\n",
    "    # Fixed the issue that the negative value of cs_map caused ms_ssim to output Nan.\n",
    "    cs_map = F.relu(cs_map)\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) / (mu1_sq + mu2_sq + C1)) * cs_map\n",
    "\n",
    "    ssim_val = ssim_map.mean(dim=(1, 2, 3))  # reduce along CHW\n",
    "    cs = cs_map.mean(dim=(1, 2, 3))\n",
    "\n",
    "    return ssim_val, cs\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def ms_ssim(X, Y, window, data_range: float, weights, use_padding: bool = False, eps: float = 1e-8):\n",
    "    '''\n",
    "    interface of ms-ssim\n",
    "    :param X: a batch of images, (N,C,H,W)\n",
    "    :param Y: a batch of images, (N,C,H,W)\n",
    "    :param window: 1-D gauss kernel\n",
    "    :param data_range: value range of input images. (usually 1.0 or 255)\n",
    "    :param weights: weights for different levels\n",
    "    :param use_padding: padding image before conv\n",
    "    :param eps: use for avoid grad nan.\n",
    "    :return:\n",
    "    '''\n",
    "    weights = weights[:, None]\n",
    "\n",
    "    levels = weights.shape[0]\n",
    "    vals = []\n",
    "    for i in range(levels):\n",
    "        ss, cs = ssim(X, Y, window=window, data_range=data_range, use_padding=use_padding)\n",
    "\n",
    "        if i < levels - 1:\n",
    "            vals.append(cs)\n",
    "            X = F.avg_pool2d(X, kernel_size=2, stride=2, ceil_mode=True)\n",
    "            Y = F.avg_pool2d(Y, kernel_size=2, stride=2, ceil_mode=True)\n",
    "        else:\n",
    "            vals.append(ss)\n",
    "\n",
    "    vals = torch.stack(vals, dim=0)\n",
    "    # Use for fix a issue. When c = a ** b and a is 0, c.backward() will cause the a.grad become inf.\n",
    "    vals = vals.clamp_min(eps)\n",
    "    # The origin ms-ssim op.\n",
    "    ms_ssim_val = torch.prod(vals[:-1] ** weights[:-1] * vals[-1:] ** weights[-1:], dim=0)\n",
    "    # The new ms-ssim op. But I don't know which is best.\n",
    "    # ms_ssim_val = torch.prod(vals ** weights, dim=0)\n",
    "    # In this file's image training demo. I feel the old ms-ssim more better. So I keep use old ms-ssim op.\n",
    "    return ms_ssim_val\n",
    "\n",
    "\n",
    "class SSIM(torch.jit.ScriptModule):\n",
    "    __constants__ = ['data_range', 'use_padding']\n",
    "\n",
    "    def __init__(self, window_size=11, window_sigma=1.5, data_range=255., channel=3, use_padding=False):\n",
    "        '''\n",
    "        :param window_size: the size of gauss kernel\n",
    "        :param window_sigma: sigma of normal distribution\n",
    "        :param data_range: value range of input images. (usually 1.0 or 255)\n",
    "        :param channel: input channels (default: 3)\n",
    "        :param use_padding: padding image before conv\n",
    "        '''\n",
    "        super().__init__()\n",
    "        assert window_size % 2 == 1, 'Window size must be odd.'\n",
    "        window = create_window(window_size, window_sigma, channel)\n",
    "        self.register_buffer('window', window)\n",
    "        self.data_range = data_range\n",
    "        self.use_padding = use_padding\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def forward(self, X, Y):\n",
    "        r = ssim(X, Y, window=self.window, data_range=self.data_range, use_padding=self.use_padding)\n",
    "        return r[0]\n",
    "\n",
    "\n",
    "class MS_SSIM(torch.jit.ScriptModule):\n",
    "    __constants__ = ['data_range', 'use_padding', 'eps']\n",
    "\n",
    "    def __init__(self, window_size=11, window_sigma=1.5, data_range=1.0, channel=3, use_padding=False, weights=None,\n",
    "                 levels=None, eps=1e-8):\n",
    "        \"\"\"\n",
    "        class for ms-ssim\n",
    "        :param window_size: the size of gauss kernel\n",
    "        :param window_sigma: sigma of normal distribution\n",
    "        :param data_range: value range of input images. (usually 1.0 or 255)\n",
    "        :param channel: input channels\n",
    "        :param use_padding: padding image before conv\n",
    "        :param weights: weights for different levels. (default [0.0448, 0.2856, 0.3001, 0.2363, 0.1333])\n",
    "        :param levels: number of downsampling\n",
    "        :param eps: Use for fix a issue. When c = a ** b and a is 0, c.backward() will cause the a.grad become inf.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert window_size % 2 == 1, 'Window size must be odd.'\n",
    "        self.data_range = data_range\n",
    "        self.use_padding = use_padding\n",
    "        self.eps = eps\n",
    "\n",
    "        window = create_window(window_size, window_sigma, channel)\n",
    "        self.register_buffer('window', window)\n",
    "\n",
    "        if weights is None:\n",
    "            weights = [0.0448, 0.2856, 0.3001, 0.2363, 0.1333]\n",
    "        weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "        if levels is not None:\n",
    "            weights = weights[:levels]\n",
    "            weights = weights / weights.sum()\n",
    "\n",
    "        self.register_buffer('weights', weights)\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def forward(self, X, Y):\n",
    "        return 1 - ms_ssim(X, Y, window=self.window, data_range=self.data_range, weights=self.weights,\n",
    "                       use_padding=self.use_padding, eps=self.eps)\n",
    "\n",
    "\n",
    "class MSE(torch.nn.Module):\n",
    "    def __init__(self, normalization=True):\n",
    "        super(MSE, self).__init__()\n",
    "        self.squared_difference = torch.nn.MSELoss(reduction='none')\n",
    "        self.normalization = normalization\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        # [-1 1] to [0 1]\n",
    "        if self.normalization:\n",
    "            X = (X + 1) / 2\n",
    "            Y = (Y + 1) / 2\n",
    "        return torch.mean(self.squared_difference(X * 255., Y * 255.))  # / 255.\n",
    "\n",
    "\n",
    "class Distortion(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Distortion, self).__init__()\n",
    "        if args.distortion_metric == 'MSE':\n",
    "            self.dist = MSE(normalization=False)\n",
    "        elif args.distortion_metric == 'SSIM':\n",
    "            self.dist = SSIM()\n",
    "        elif args.distortion_metric == 'MS-SSIM':\n",
    "            if args.trainset == 'CIFAR10':\n",
    "                self.dist = MS_SSIM(window_size=3, data_range=1., levels=4, channel=3).cuda()\n",
    "            else:\n",
    "                self.dist = MS_SSIM(data_range=1., levels=4, channel=3).cuda()\n",
    "        else:\n",
    "            args.logger.info(\"Unknown distortion type!\")\n",
    "            raise ValueError\n",
    "\n",
    "    def forward(self, X, Y, normalization=False):\n",
    "        return self.dist.forward(X, Y).mean()  # / 255."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# modules for the later encoder and decoder network\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, add_token=True, token_num=0, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))  # (N+1)x(N+1)\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "\n",
    "        if add_token:\n",
    "            attn[:, :, token_num:, token_num:] = attn[:, :, token_num:, token_num:] + relative_position_bias.unsqueeze(\n",
    "                0)\n",
    "        else:\n",
    "            attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            if add_token:\n",
    "                # padding mask matrix\n",
    "                mask = F.pad(mask, (token_num, 0, token_num, 0), \"constant\", 0)\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, out_dim=None, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        if out_dim is None:\n",
    "            out_dim = dim\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, out_dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "        # self.proj = nn.Conv2d(dim, out_dim, kernel_size=2, stride=2)\n",
    "        # self.norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        # print(x.shape)\n",
    "        # print(self.input_resolution)\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "        x = x.view(B, H, W, C)\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, H*W//4, 4 * C)  # B H/2*W/2 4*C\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        # x = x.view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        # x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        # x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchMerging4x(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm, use_conv=False):\n",
    "        super().__init__()\n",
    "        H, W = input_resolution\n",
    "        self.patch_merging1 = PatchMerging((H, W), dim, norm_layer=nn.LayerNorm, use_conv=use_conv)\n",
    "        self.patch_merging2 = PatchMerging((H // 2, W // 2), dim, norm_layer=nn.LayerNorm, use_conv=use_conv)\n",
    "\n",
    "    def forward(self, x, H=None, W=None):\n",
    "        if H is None:\n",
    "            H, W = self.input_resolution\n",
    "        x = self.patch_merging1(x, H, W)\n",
    "        x = self.patch_merging2(x, H//2, W//2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchReverseMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, out_dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.out_dim = out_dim\n",
    "        self.increment = nn.Linear(dim, out_dim * 4, bias=False)\n",
    "        self.norm = norm_layer(dim)\n",
    "        # self.proj = nn.ConvTranspose2d(dim // 4, 3, 3, stride=1, padding=1)\n",
    "        # self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "        x = self.norm(x)\n",
    "        x = self.increment(x)\n",
    "        x = x.view(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "        x = nn.PixelShuffle(2)(x)\n",
    "        # x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        # x = self.norm(x)\n",
    "        # print(x.shape)\n",
    "        x = x.flatten(2).permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * 2 * W * 2 * self.dim // 4\n",
    "        flops += (H * 2) * (W * 2) * self.dim // 4 * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchReverseMerging4x(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm, use_conv=False):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        H, W = input_resolution\n",
    "        self.patch_reverse_merging1 = PatchReverseMerging((H, W), dim, norm_layer=nn.LayerNorm, use_conv=use_conv)\n",
    "        self.patch_reverse_merging2 = PatchReverseMerging((H * 2, W * 2), dim, norm_layer=nn.LayerNorm, use_conv=use_conv)\n",
    "\n",
    "    def forward(self, x, H=None, W=None):\n",
    "        if H is None:\n",
    "            H, W = self.input_resolution\n",
    "        x = self.patch_reverse_merging1(x, H, W)\n",
    "        x = self.patch_reverse_merging2(x, H*2, W*2)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * 2 * W * 2 * self.dim // 4\n",
    "        flops += (H * 2) * (W * 2) * self.dim // 4 * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        # assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "        #     f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        return flops"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# encoder network\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale)\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "        B_, N, C = x_windows.shape\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = self.attn(x_windows,\n",
    "                                 add_token=False,\n",
    "                                 mask=self.attn_mask)\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + x\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops\n",
    "\n",
    "    def update_mask(self):\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "            self.attn_mask = attn_mask.cuda()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    def __init__(self, dim, out_dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, norm_layer=nn.LayerNorm,\n",
    "                 downsample=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=out_dim,\n",
    "                                 input_resolution=(input_resolution[0] // 2, input_resolution[1] // 2),\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, out_dim=out_dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        for _, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "    def update_resolution(self, H, W):\n",
    "        for _, blk in enumerate(self.blocks):\n",
    "            blk.input_resolution = (H, W)\n",
    "            blk.update_mask()\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.input_resolution = (H * 2, W * 2)\n",
    "\n",
    "class AdaptiveModulator(nn.Module):\n",
    "    def __init__(self, M):\n",
    "        super(AdaptiveModulator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, M),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(M, M),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(M, M),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, snr):\n",
    "        return self.fc(snr)\n",
    "\n",
    "class WITT_Encoder(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_chans,\n",
    "                 embed_dims, depths, num_heads, C,\n",
    "                 window_size=4, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
    "                 bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(depths)\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = bottleneck_dim\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.embed_dims = embed_dims\n",
    "        self.in_chans = in_chans\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = img_size\n",
    "        self.H = img_size[0] // (2 ** self.num_layers)\n",
    "        self.W = img_size[1] // (2 ** self.num_layers)\n",
    "        self.patch_embed = PatchEmbed(img_size, 2, 3, embed_dims[0])\n",
    "        self.hidden_dim = int(self.embed_dims[len(embed_dims)-1] * 1.5)\n",
    "        self.layer_num = layer_num = 7\n",
    "        self.bm_list = nn.ModuleList()\n",
    "        self.sm_list = nn.ModuleList()\n",
    "        self.sm_list.append(nn.Linear(self.embed_dims[len(embed_dims)-1], self.hidden_dim))\n",
    "        for i in range(layer_num):\n",
    "            if i == layer_num - 1:\n",
    "                outdim = self.embed_dims[len(embed_dims)-1]\n",
    "            else:\n",
    "                outdim = self.hidden_dim\n",
    "            self.bm_list.append(AdaptiveModulator(self.hidden_dim))\n",
    "            self.sm_list.append(nn.Linear(self.hidden_dim, outdim))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dims[i_layer - 1]) if i_layer != 0 else 3,\n",
    "                               out_dim=int(embed_dims[i_layer]),\n",
    "                               input_resolution=(self.patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 self.patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if i_layer != 0 else None)\n",
    "            print(\"Encoder \", layer.extra_repr())\n",
    "            self.layers.append(layer)\n",
    "        self.norm = norm_layer(embed_dims[-1])\n",
    "        self.head_list = nn.Linear(embed_dims[-1], C)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x, snr, model):\n",
    "        B, C, H, W = x.size()\n",
    "        device = x.get_device()\n",
    "        x = self.patch_embed(x)\n",
    "        for i_layer, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if model == 'WITT':\n",
    "            snr_cuda = torch.tensor(snr, dtype=torch.float).to(device)\n",
    "            snr_batch = snr_cuda.unsqueeze(0).expand(B, -1)\n",
    "            for i in range(self.layer_num):\n",
    "                if i == 0:\n",
    "                    temp = self.sm_list[i](x.detach())\n",
    "                else:\n",
    "                    temp = self.sm_list[i](temp)\n",
    "\n",
    "                bm = self.bm_list[i](snr_batch).unsqueeze(1).expand(-1, H * W // (self.num_layers ** 4), -1)\n",
    "                temp = temp * bm\n",
    "            mod_val = self.sigmoid(self.sm_list[-1](temp))\n",
    "            x = x * mod_val\n",
    "\n",
    "        x = self.head_list(x)\n",
    "        return x\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "        return flops\n",
    "\n",
    "    def update_resolution(self, H, W):\n",
    "        self.input_resolution = (H, W)\n",
    "        for i_layer, layer in enumerate(self.layers):\n",
    "            layer.update_resolution(H // (2 ** (i_layer + 1)),\n",
    "                                    W // (2 ** (i_layer + 1)))\n",
    "\n",
    "\n",
    "def create_encoder(**kwargs):\n",
    "    model = WITT_Encoder(**kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(config):\n",
    "    input_image = torch.ones([1, 256, 256]).to(config.device)\n",
    "    model = create_encoder(**config.encoder_kwargs)\n",
    "    model(input_image)\n",
    "    num_params = 0\n",
    "    for param in model.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(\"TOTAL Params {}M\".format(num_params / 10 ** 6))\n",
    "    print(\"TOTAL FLOPs {}G\".format(model.flops() / 10 ** 9))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# decoder network\n",
    "class BasicLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, out_dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 norm_layer=nn.LayerNorm, upsample=None,):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if upsample is not None:\n",
    "            self.upsample = upsample(input_resolution, dim=dim, out_dim=out_dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.upsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for _, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "            print(\"blk.flops()\", blk.flops())\n",
    "        if self.upsample is not None:\n",
    "            flops += self.upsample.flops()\n",
    "            print(\"upsample.flops()\", self.upsample.flops())\n",
    "        return flops\n",
    "\n",
    "    def update_resolution(self, H, W):\n",
    "        self.input_resolution = (H, W)\n",
    "        for _, blk in enumerate(self.blocks):\n",
    "            blk.input_resolution = (H, W)\n",
    "            blk.update_mask()\n",
    "        if self.upsample is not None:\n",
    "            self.upsample.input_resolution = (H, W)\n",
    "\n",
    "\n",
    "class WITT_Decoder(nn.Module):\n",
    "    def __init__(self, img_size, embed_dims, depths, num_heads, C,\n",
    "                 window_size=4, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = len(depths)\n",
    "        self.ape = ape\n",
    "        self.embed_dims = embed_dims\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = bottleneck_dim\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.H = img_size[0]\n",
    "        self.W = img_size[1]\n",
    "        self.patches_resolution = (img_size[0] // 2 ** len(depths), img_size[1] // 2 ** len(depths))\n",
    "        num_patches = self.H // 4 * self.W // 4\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dims[0]))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dims[i_layer]),\n",
    "                               out_dim=int(embed_dims[i_layer + 1]) if (i_layer < self.num_layers - 1) else 3,\n",
    "                               input_resolution=(self.patches_resolution[0] * (2 ** i_layer),\n",
    "                                                 self.patches_resolution[1] * (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               norm_layer=norm_layer,\n",
    "                               upsample=PatchReverseMerging)\n",
    "            self.layers.append(layer)\n",
    "            print(\"Decoder \", layer.extra_repr())\n",
    "        self.head_list = nn.Linear(C, embed_dims[0])\n",
    "        self.apply(self._init_weights)\n",
    "        self.hidden_dim = int(self.embed_dims[0] * 1.5)\n",
    "        self.layer_num = layer_num = 7\n",
    "        self.bm_list = nn.ModuleList()\n",
    "        self.sm_list = nn.ModuleList()\n",
    "        self.sm_list.append(nn.Linear(self.embed_dims[0], self.hidden_dim))\n",
    "        for i in range(layer_num):\n",
    "            if i == layer_num - 1:\n",
    "                outdim = self.embed_dims[0]\n",
    "            else:\n",
    "                outdim = self.hidden_dim\n",
    "            self.bm_list.append(AdaptiveModulator(self.hidden_dim))\n",
    "            self.sm_list.append(nn.Linear(self.hidden_dim, outdim))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, snr, model):\n",
    "        B, L, C = x.size()\n",
    "        device = x.get_device()\n",
    "        x = self.head_list(x)\n",
    "\n",
    "        if model == 'WITT':\n",
    "            # token modulation according to input snr value\n",
    "            snr_cuda = torch.tensor(snr, dtype=torch.float).to(device)\n",
    "            snr_batch = snr_cuda.unsqueeze(0).expand(B, -1)\n",
    "            for i in range(self.layer_num):\n",
    "                if i == 0:\n",
    "                    temp = self.sm_list[i](x.detach())\n",
    "                else:\n",
    "                    temp = self.sm_list[i](temp)\n",
    "                bm = self.bm_list[i](snr_batch).unsqueeze(1).expand(-1, L, -1)\n",
    "                temp = temp * bm\n",
    "            mod_val = self.sigmoid(self.sm_list[-1](temp))\n",
    "            x = x * mod_val\n",
    "\n",
    "        for i_layer, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        B, L, N = x.shape\n",
    "        x = x.reshape(B, self.H, self.W, N).permute(0, 3, 1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        return flops\n",
    "\n",
    "    def update_resolution(self, H, W):\n",
    "        self.input_resolution = (H, W)\n",
    "        self.H = H * 2 ** len(self.layers)\n",
    "        self.W = W * 2 ** len(self.layers)\n",
    "        for i_layer, layer in enumerate(self.layers):\n",
    "            layer.update_resolution(H * (2 ** i_layer),\n",
    "                                    W * (2 ** i_layer))\n",
    "\n",
    "\n",
    "def create_decoder(**kwargs):\n",
    "    model = WITT_Decoder(**kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def build_model(config):\n",
    "    input_image = torch.ones([1, 1536, 256]).to(config.device)\n",
    "    model = create_decoder(**config.encoder_kwargs).to(config.device)\n",
    "    t0 = datetime.datetime.now()\n",
    "    with torch.no_grad():\n",
    "        for i in range(100):\n",
    "            features = model(input_image, SNR=15)\n",
    "        t1 = datetime.datetime.now()\n",
    "        delta_t = t1 - t0\n",
    "        print(\"Decoding Time per img {}s\".format((delta_t.seconds + 1e-6 * delta_t.microseconds) / 100))\n",
    "    print(\"TOTAL FLOPs {}G\".format(model.flops() / 10 ** 9))\n",
    "    num_params = 0\n",
    "    for param in model.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(\"TOTAL Params {}M\".format(num_params / 10 ** 6))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataset\n",
    "NUM_DATASET_WORKERS = 0\n",
    "SCALE_MIN = 0.75\n",
    "SCALE_MAX = 0.95\n",
    "\n",
    "\n",
    "class HR_image(Dataset):\n",
    "    files = {\"train\": \"train\", \"test\": \"test\", \"val\": \"validation\"}\n",
    "\n",
    "    def __init__(self, config, data_dir):\n",
    "        self.imgs = []\n",
    "        for dir in data_dir:\n",
    "            self.imgs += glob(os.path.join(dir, '*.jpg'))\n",
    "            self.imgs += glob(os.path.join(dir, '*.png'))\n",
    "        _, self.im_height, self.im_width = config.image_dims\n",
    "        self.crop_size = self.im_height\n",
    "        self.image_dims = (3, self.im_height, self.im_width)\n",
    "        self.transform = self._transforms()\n",
    "\n",
    "    def _transforms(self,):\n",
    "        \"\"\"\n",
    "        Up(down)scale and randomly crop to `crop_size` x `crop_size`\n",
    "        \"\"\"\n",
    "        transforms_list = [\n",
    "            transforms.RandomCrop((self.im_height, self.im_width)),\n",
    "            transforms.ToTensor()]\n",
    "\n",
    "        return transforms.Compose(transforms_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert('RGB')\n",
    "        transformed = self.transform(img)\n",
    "        return transformed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "class Datasets(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.imgs = []\n",
    "        for dir in self.data_dir:\n",
    "            self.imgs += glob(os.path.join(dir, '*.jpg'))\n",
    "            self.imgs += glob(os.path.join(dir, '*.png'))\n",
    "        self.imgs.sort()\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image_ori = self.imgs[item]\n",
    "        image = Image.open(image_ori).convert('RGB')\n",
    "        self.im_height, self.im_width = image.size\n",
    "        if self.im_height % 128 != 0 or self.im_width % 128 != 0:\n",
    "            self.im_height = self.im_height - self.im_height % 128\n",
    "            self.im_width = self.im_width - self.im_width % 128\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.CenterCrop((self.im_width, self.im_height)),\n",
    "            transforms.ToTensor()])\n",
    "        img = self.transform(image)\n",
    "        return img\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "class CIFAR10(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.len = dataset.__len__()\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.dataset.__getitem__(item % self.len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len * 10\n",
    "\n",
    "\n",
    "def get_loader(args, config):\n",
    "    if args.trainset == 'DIV2K':\n",
    "        train_dataset = HR_image(config, config.train_data_dir)\n",
    "        test_dataset = Datasets(config.test_data_dir)\n",
    "    elif args.trainset == 'CIFAR10':\n",
    "        dataset_ = datasets.CIFAR10\n",
    "        if config.norm is True:\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        else:\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor()])\n",
    "\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor()])\n",
    "        train_dataset = dataset_(root=config.train_data_dir,\n",
    "                                 train=True,\n",
    "                                 transform=transform_train,\n",
    "                                 download=True)\n",
    "\n",
    "        test_dataset = dataset_(root=config.test_data_dir,\n",
    "                                train=False,\n",
    "                                transform=transform_test,\n",
    "                                download=True)\n",
    "\n",
    "        train_dataset = CIFAR10(train_dataset)\n",
    "\n",
    "    else:\n",
    "        train_dataset = Datasets(config.train_data_dir)\n",
    "        test_dataset = Datasets(config.test_data_dir)\n",
    "\n",
    "    def worker_init_fn_seed(worker_id):\n",
    "        seed = 10\n",
    "        seed += worker_id\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               num_workers=NUM_DATASET_WORKERS,\n",
    "                                               pin_memory=True,\n",
    "                                               batch_size=config.batch_size,\n",
    "                                               worker_init_fn=worker_init_fn_seed,\n",
    "                                               shuffle=True,\n",
    "                                               drop_last=True)\n",
    "    if args.trainset == 'CIFAR10':\n",
    "        test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                  batch_size=1024,\n",
    "                                  shuffle=False)\n",
    "\n",
    "    else:\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                              batch_size=1,\n",
    "                                              shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# semantic communication network\n",
    "class WITT(nn.Module):\n",
    "    def __init__(self, args, config):\n",
    "        super(WITT, self).__init__()\n",
    "        self.config = config\n",
    "        encoder_kwargs = config.encoder_kwargs\n",
    "        decoder_kwargs = config.decoder_kwargs\n",
    "        self.encoder = create_encoder(**encoder_kwargs)\n",
    "        self.decoder = create_decoder(**decoder_kwargs)\n",
    "        if config.logger is not None:\n",
    "            config.logger.info(\"Network config: \")\n",
    "            config.logger.info(\"Encoder: \")\n",
    "            config.logger.info(encoder_kwargs)\n",
    "            config.logger.info(\"Decoder: \")\n",
    "            config.logger.info(decoder_kwargs)\n",
    "        self.distortion_loss = Distortion(args)\n",
    "        self.channel = Channel(args, config)\n",
    "        self.pass_channel = config.pass_channel\n",
    "        self.squared_difference = torch.nn.MSELoss(reduction='none')\n",
    "        self.H = self.W = 0\n",
    "        self.multiple_snr = args.multiple_snr.split(\",\")\n",
    "        for i in range(len(self.multiple_snr)):\n",
    "            self.multiple_snr[i] = int(self.multiple_snr[i])\n",
    "        self.downsample = config.downsample\n",
    "        self.model = args.model\n",
    "\n",
    "    def distortion_loss_wrapper(self, x_gen, x_real):\n",
    "        distortion_loss = self.distortion_loss.forward(x_gen, x_real, normalization=self.config.norm)\n",
    "        return distortion_loss\n",
    "\n",
    "    def feature_pass_channel(self, feature, chan_param, avg_pwr=False):\n",
    "        noisy_feature = self.channel.forward(feature, chan_param, avg_pwr)\n",
    "        return noisy_feature\n",
    "\n",
    "    def forward(self, input_image, given_SNR = None):\n",
    "        B, _, H, W = input_image.shape\n",
    "\n",
    "        if H != self.H or W != self.W:\n",
    "            self.encoder.update_resolution(H, W)\n",
    "            self.decoder.update_resolution(H // (2 ** self.downsample), W // (2 ** self.downsample))\n",
    "            self.H = H\n",
    "            self.W = W\n",
    "\n",
    "        if given_SNR is None:\n",
    "            SNR = choice(self.multiple_snr)\n",
    "            chan_param = SNR\n",
    "        else:\n",
    "            chan_param = given_SNR\n",
    "\n",
    "        feature = self.encoder(input_image, chan_param, self.model)\n",
    "\n",
    "\n",
    "        CBR = feature.numel() / 2 / input_image.numel()\n",
    "        # Feature pass channel\n",
    "        if self.pass_channel:\n",
    "            noisy_feature = self.feature_pass_channel(feature, chan_param)\n",
    "        else:\n",
    "            noisy_feature = feature\n",
    "\n",
    "        recon_image = self.decoder(noisy_feature, chan_param, self.model)\n",
    "        mse = self.squared_difference(input_image * 255., recon_image.clamp(0., 1.) * 255.)\n",
    "        loss_G = self.distortion_loss.forward(input_image, recon_image.clamp(0., 1.))\n",
    "        return recon_image, CBR, chan_param, mse.mean(), loss_G.mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# utils\n",
    "class AverageMeter:\n",
    "    \"\"\"Compute running average.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def clear(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "\n",
    "def logger_configuration(config, save_log=False, test_mode=False):\n",
    "    # 配置 logger\n",
    "    logger = logging.getLogger(\"Deep joint source channel coder\")\n",
    "    if test_mode:\n",
    "        config.workdir += '_test'\n",
    "    if save_log:\n",
    "        makedirs(config.workdir)\n",
    "        makedirs(config.samples)\n",
    "        makedirs(config.models)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s] %(message)s')\n",
    "    stdhandler = logging.StreamHandler()\n",
    "    stdhandler.setLevel(logging.INFO)\n",
    "    stdhandler.setFormatter(formatter)\n",
    "    logger.addHandler(stdhandler)\n",
    "    if save_log:\n",
    "        filehandler = logging.FileHandler(config.log)\n",
    "        filehandler.setLevel(logging.INFO)\n",
    "        filehandler.setFormatter(formatter)\n",
    "        logger.addHandler(filehandler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    config.logger = logger\n",
    "    return config.logger\n",
    "\n",
    "def makedirs(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def save_model(model, save_path):\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "\n",
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train\n",
    "torch.backends.cudnn.benchmark = True\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "parser = argparse.ArgumentParser(description='WITT')\n",
    "parser.add_argument('--training', action='store_true',\n",
    "                    help='training or testing')\n",
    "parser.add_argument('--trainset', type=str, default='DIV2K',\n",
    "                    choices=['CIFAR10', 'CIFAR10'],\n",
    "                    help='train dataset name')\n",
    "parser.add_argument('--testset', type=str, default='kodak',\n",
    "                    choices=['kodak', 'CLIC21'],\n",
    "                    help='specify the testset for HR models')\n",
    "parser.add_argument('--distortion-metric', type=str, default='MSE',\n",
    "                    choices=['MSE', 'MS-SSIM'],\n",
    "                    help='evaluation metrics')\n",
    "parser.add_argument('--model', type=str, default='WITT_W/O',\n",
    "                    choices=['WITT', 'WITT_W/O'],\n",
    "                    help='WITT model or WITT without channel ModNet')\n",
    "parser.add_argument('--channel-type', type=str, default='awgn',\n",
    "                    choices=['awgn', 'rayleigh'],\n",
    "                    help='wireless channel model, awgn or rayleigh')\n",
    "parser.add_argument('--C', type=int, default=96,\n",
    "                    help='bottleneck dimension')\n",
    "parser.add_argument('--multiple-snr', type=str, default='10',\n",
    "                    help='random or fixed snr')\n",
    "args = parser.parse_args()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class config():\n",
    "    seed = 1024\n",
    "    pass_channel = True\n",
    "    CUDA = True\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    norm = False\n",
    "    # logger\n",
    "    print_step = 100\n",
    "    plot_step = 10000\n",
    "    filename = datetime.now().__str__()[:-7]\n",
    "    filename = 1_11\n",
    "    workdir = './history/{}'.format(filename)\n",
    "    log = workdir + '/Log_{}.log'.format(filename)\n",
    "    samples = workdir + '/samples'\n",
    "    models = workdir + '/models'\n",
    "    logger = None\n",
    "\n",
    "    # training details\n",
    "    normalize = False\n",
    "    learning_rate = 0.0001\n",
    "    tot_epoch = 10000000\n",
    "\n",
    "    if args.trainset == 'CIFAR10':\n",
    "        save_model_freq = 5\n",
    "        image_dims = (3, 32, 32)\n",
    "        train_data_dir = \"data/cifar\"\n",
    "        test_data_dir = \"data/cifar\"\n",
    "        batch_size = 128\n",
    "        downsample = 2\n",
    "        encoder_kwargs = dict(\n",
    "            img_size=(image_dims[1], image_dims[2]), patch_size=2, in_chans=3,\n",
    "            embed_dims=[128, 256], depths=[2, 4], num_heads=[4, 8], C=args.C,\n",
    "            window_size=2, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "            norm_layer=nn.LayerNorm, patch_norm=True,\n",
    "        )\n",
    "        decoder_kwargs = dict(\n",
    "            img_size=(image_dims[1], image_dims[2]),\n",
    "            embed_dims=[256, 128], depths=[4, 2], num_heads=[8, 4], C=args.C,\n",
    "            window_size=2, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "            norm_layer=nn.LayerNorm, patch_norm=True,\n",
    "        )\n",
    "    elif args.trainset == 'DIV2K':\n",
    "        save_model_freq = 100\n",
    "        image_dims = (3, 256, 256)\n",
    "        train_data_dir = [\"/media/Dataset/HR_Image_dataset/\"]\n",
    "        if args.testset == 'kodak':\n",
    "            test_data_dir = [\"/media/Dataset/kodak_test/\"]\n",
    "        elif args.testset == 'CLIC21':\n",
    "            test_data_dir = [\"/media/Dataset/CLIC21/\"]\n",
    "        batch_size = 16\n",
    "        downsample = 4\n",
    "        encoder_kwargs = dict(\n",
    "            img_size=(image_dims[1], image_dims[2]), patch_size=2, in_chans=3,\n",
    "            embed_dims=[128, 192, 256, 320], depths=[2, 2, 6, 2], num_heads=[4, 6, 8, 10],\n",
    "            C=args.C, window_size=8, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "            norm_layer=nn.LayerNorm, patch_norm=True,\n",
    "        )\n",
    "        decoder_kwargs = dict(\n",
    "            img_size=(image_dims[1], image_dims[2]),\n",
    "            embed_dims=[320, 256, 192, 128], depths=[2, 6, 2, 2], num_heads=[10, 8, 6, 4],\n",
    "            C=args.C, window_size=8, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "            norm_layer=nn.LayerNorm, patch_norm=True,\n",
    "        )\n",
    "\n",
    "\n",
    "if args.trainset == 'CIFAR10':\n",
    "    CalcuSSIM = MS_SSIM(window_size=3, data_range=1., levels=4, channel=3).cuda()\n",
    "else:\n",
    "    CalcuSSIM = MS_SSIM(data_range=1., levels=4, channel=3).cuda()\n",
    "\n",
    "def load_weights(model_path):\n",
    "    pretrained = torch.load(model_path)\n",
    "    net.load_state_dict(pretrained, strict=True)\n",
    "    del pretrained\n",
    "\n",
    "\n",
    "def train_one_epoch(args):\n",
    "    net.train()\n",
    "    elapsed, losses, psnrs, msssims, cbrs, snrs = [AverageMeter() for _ in range(6)]\n",
    "    metrics = [elapsed, losses, psnrs, msssims, cbrs, snrs]\n",
    "    global global_step\n",
    "    if args.trainset == 'CIFAR10':\n",
    "        for batch_idx, (input, label) in enumerate(train_loader):\n",
    "            start_time = time.time()\n",
    "            global_step += 1\n",
    "            input = input.cuda()\n",
    "            recon_image, CBR, SNR, mse, loss_G = net(input)\n",
    "            loss = loss_G\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            elapsed.update(time.time() - start_time)\n",
    "            losses.update(loss.item())\n",
    "            cbrs.update(CBR)\n",
    "            snrs.update(SNR)\n",
    "            if mse.item() > 0:\n",
    "                psnr = 10 * (torch.log(255. * 255. / mse) / np.log(10))\n",
    "                psnrs.update(psnr.item())\n",
    "                msssim = 1 - CalcuSSIM(input, recon_image.clamp(0., 1.)).mean().item()\n",
    "                msssims.update(msssim)\n",
    "            else:\n",
    "                psnrs.update(100)\n",
    "                msssims.update(100)\n",
    "\n",
    "            if (global_step % config.print_step) == 0:\n",
    "                process = (global_step % train_loader.__len__()) / (train_loader.__len__()) * 100.0\n",
    "                log = (' | '.join([\n",
    "                    f'Epoch {epoch}',\n",
    "                    f'Step [{global_step % train_loader.__len__()}/{train_loader.__len__()}={process:.2f}%]',\n",
    "                    f'Time {elapsed.val:.3f}',\n",
    "                    f'Loss {losses.val:.3f} ({losses.avg:.3f})',\n",
    "                    f'CBR {cbrs.val:.4f} ({cbrs.avg:.4f})',\n",
    "                    f'SNR {snrs.val:.1f} ({snrs.avg:.1f})',\n",
    "                    f'PSNR {psnrs.val:.3f} ({psnrs.avg:.3f})',\n",
    "                    f'MSSSIM {msssims.val:.3f} ({msssims.avg:.3f})',\n",
    "                    f'Lr {cur_lr}',\n",
    "                ]))\n",
    "                logger.info(log)\n",
    "                for i in metrics:\n",
    "                    i.clear()\n",
    "    else:\n",
    "        for batch_idx, input in enumerate(train_loader):\n",
    "            start_time = time.time()\n",
    "            global_step += 1\n",
    "            input = input.cuda()\n",
    "            recon_image, CBR, SNR, mse, loss_G = net(input)\n",
    "            loss = loss_G\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            elapsed.update(time.time() - start_time)\n",
    "            losses.update(loss.item())\n",
    "            cbrs.update(CBR)\n",
    "            snrs.update(SNR)\n",
    "            if mse.item() > 0:\n",
    "                psnr = 10 * (torch.log(255. * 255. / mse) / np.log(10))\n",
    "                psnrs.update(psnr.item())\n",
    "                msssim = 1 - loss_G\n",
    "                msssims.update(msssim)\n",
    "\n",
    "            else:\n",
    "                psnrs.update(100)\n",
    "                msssims.update(100)\n",
    "\n",
    "            if (global_step % config.print_step) == 0:\n",
    "                process = (global_step % train_loader.__len__()) / (train_loader.__len__()) * 100.0\n",
    "                log = (' | '.join([\n",
    "                    f'Epoch {epoch}',\n",
    "                    f'Step [{global_step % train_loader.__len__()}/{train_loader.__len__()}={process:.2f}%]',\n",
    "                    f'Time {elapsed.val:.3f}',\n",
    "                    f'Loss {losses.val:.3f} ({losses.avg:.3f})',\n",
    "                    f'CBR {cbrs.val:.4f} ({cbrs.avg:.4f})',\n",
    "                    f'SNR {snrs.val:.1f} ({snrs.avg:.1f})',\n",
    "                    f'PSNR {psnrs.val:.3f} ({psnrs.avg:.3f})',\n",
    "                    f'MSSSIM {msssims.val:.3f} ({msssims.avg:.3f})',\n",
    "                    f'Lr {cur_lr}',\n",
    "                ]))\n",
    "                logger.info(log)\n",
    "                for i in metrics:\n",
    "                    i.clear()\n",
    "    for i in metrics:\n",
    "        i.clear()\n",
    "\n",
    "def test():\n",
    "    config.isTrain = False\n",
    "    net.eval()\n",
    "    elapsed, psnrs, msssims, snrs, cbrs = [AverageMeter() for _ in range(5)]\n",
    "    metrics = [elapsed, psnrs, msssims, snrs, cbrs]\n",
    "    multiple_snr = args.multiple_snr.split(\",\")\n",
    "    for i in range(len(multiple_snr)):\n",
    "        multiple_snr[i] = int(multiple_snr[i])\n",
    "    results_snr = np.zeros(len(multiple_snr))\n",
    "    results_cbr = np.zeros(len(multiple_snr))\n",
    "    results_psnr = np.zeros(len(multiple_snr))\n",
    "    results_msssim = np.zeros(len(multiple_snr))\n",
    "    for i, SNR in enumerate(multiple_snr):\n",
    "        with torch.no_grad():\n",
    "            if args.trainset == 'CIFAR10':\n",
    "                for batch_idx, (input, label) in enumerate(test_loader):\n",
    "                    start_time = time.time()\n",
    "                    input = input.cuda()\n",
    "                    recon_image, CBR, SNR, mse, loss_G = net(input, SNR)\n",
    "                    elapsed.update(time.time() - start_time)\n",
    "                    cbrs.update(CBR)\n",
    "                    snrs.update(SNR)\n",
    "                    if mse.item() > 0:\n",
    "                        psnr = 10 * (torch.log(255. * 255. / mse) / np.log(10))\n",
    "                        psnrs.update(psnr.item())\n",
    "                        msssim = 1 - CalcuSSIM(input, recon_image.clamp(0., 1.)).mean().item()\n",
    "                        msssims.update(msssim)\n",
    "                    else:\n",
    "                        psnrs.update(100)\n",
    "                        msssims.update(100)\n",
    "\n",
    "                    log = (' | '.join([\n",
    "                        f'Time {elapsed.val:.3f}',\n",
    "                        f'CBR {cbrs.val:.4f} ({cbrs.avg:.4f})',\n",
    "                        f'SNR {snrs.val:.1f}',\n",
    "                        f'PSNR {psnrs.val:.3f} ({psnrs.avg:.3f})',\n",
    "                        f'MSSSIM {msssims.val:.3f} ({msssims.avg:.3f})',\n",
    "                        f'Lr {cur_lr}',\n",
    "                    ]))\n",
    "                    logger.info(log)\n",
    "            else:\n",
    "                for batch_idx, input in enumerate(test_loader):\n",
    "                    start_time = time.time()\n",
    "                    input = input.cuda()\n",
    "                    recon_image, CBR, SNR, mse, loss_G = net(input, SNR)\n",
    "                    elapsed.update(time.time() - start_time)\n",
    "                    cbrs.update(CBR)\n",
    "                    snrs.update(SNR)\n",
    "                    if mse.item() > 0:\n",
    "                        psnr = 10 * (torch.log(255. * 255. / mse) / np.log(10))\n",
    "                        psnrs.update(psnr.item())\n",
    "                        msssim = 1 - CalcuSSIM(input, recon_image.clamp(0., 1.)).mean().item()\n",
    "                        msssims.update(msssim)\n",
    "                    else:\n",
    "                        psnrs.update(100)\n",
    "                        msssims.update(100)\n",
    "\n",
    "                    log = (' | '.join([\n",
    "                        f'Time {elapsed.val:.3f}',\n",
    "                        f'CBR {cbrs.val:.4f} ({cbrs.avg:.4f})',\n",
    "                        f'SNR {snrs.val:.1f}',\n",
    "                        f'PSNR {psnrs.val:.3f} ({psnrs.avg:.3f})',\n",
    "                        f'MSSSIM {msssims.val:.3f} ({msssims.avg:.3f})',\n",
    "                        f'Lr {cur_lr}',\n",
    "                    ]))\n",
    "                    logger.info(log)\n",
    "        results_snr[i] = snrs.avg\n",
    "        results_cbr[i] = cbrs.avg\n",
    "        results_psnr[i] = psnrs.avg\n",
    "        results_msssim[i] = msssims.avg\n",
    "        for t in metrics:\n",
    "            t.clear()\n",
    "\n",
    "    print(\"SNR: {}\" .format(results_snr.tolist()))\n",
    "    print(\"CBR: {}\".format(results_cbr.tolist()))\n",
    "    print(\"PSNR: {}\" .format(results_psnr.tolist()))\n",
    "    print(\"MS-SSIM: {}\".format(results_msssim.tolist()))\n",
    "    print(\"Finish Test!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
